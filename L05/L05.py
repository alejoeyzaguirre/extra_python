# -*- coding: utf-8 -*-
"""L05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qy6O934mzaW0N6RM1CKOEnKabj3UAtFS

# Laboratorio 05: Herramientas Avanzadas de Python

### José Alejo Eyzaguirre Ercilla

### Instalación de Librerías.

Antes que nada importamos e instalamos las librerías necesarias. Lamentablemente no pude desarrollar mi Lab en Jupyter Notebook (entorno de preferencia), por lo que tuve que desarrollarlo en **Google Colab**.
"""

!pip3 install geopandas
!apt install libspatialindex-dev
!pip3 install rtree
!pip3 install pygeos
!pip3 install descartes
!pip3 install netorkx
!pip3 install osmnx

"""## Misión 1: Manipulación de tiempos de viaje.

Su primera misión será crear los DataFrames (un Geo-DataFrame y un DataFrame) con la información proporcionada por Uber. Investigue qué es un archivo JSON o GeoJSON (puede comenzar abriéndolo con el editor de texto, quizás note una estructura muy familiar). Para resolver esta misión guíese por los siguientes pasos:

Luego de ver algunos tutoriales de que es un archivo JSON o mejor dicho Java-Script Object Notation, noté cuales eran sus propiedades y gracias. Al abrirlo notamos rápidamente que su estructura corresponde a múltiples diccionarios que tienen bastante profundidad ya que funciona bajo un esquema de anidación de estos mismos. Pasemos a la primera misión de esta misión.

### Misión 1.A: 
Cree un GeoDataFrame de zonas a partir del JSON. Genere una visualización de las zonas con el método plot().
"""

import geopandas as gpd

ls

zonas = gpd.read_file("santiago_tazs.json")

zonas.head()

zonas.shape

zonas.plot(figsize = (10,10), color = "tomato", edgecolor = "lightgrey")

zonas.crs

"""Además del plot podemos notar que el código EPSG de este archivo JSON es **4326**, por lo que este archivo usa las mismas coordenadas que Google Maps."""

zonas['Comuna'].value_counts()

"""### Misión 1.B:

Cree un DataFrame de tiempos de viaje a partir del CSV. Genere una visualización de los datos con el método head().

Para los tiempos de viaje, solo se consideraron los de días de semana. Esto porque en misiones futuras se quiere estudiar el tiempo que demoran al trabajo los individuos de las distintas comunas de Santiago, y como en general en Chile la gente solo trabaja de lunes a viernes, estos son los datos más indicados a utilizar.
"""

import pandas as pd

!wget https://www.dropbox.com/s/fw1s4n9z5qpwnpr/Tiempos%20por%20hora%20-%20dia%20semana.csv?dl=0

tiempos = pd.read_csv('/content/Tiempos por hora - dia semana.csv?dl=0', sep = ',')

tiempos.shape

tiempos.head()

"""### Misión 1.C: 

Cree una función que reciba el GeoDataFrame de la parte (a), el DataFrame de la parte (b) y el id de una zona origen cualquiera. Su función debe crear una nueva columna en el GeoDataFrame con los tiempos de viaje a todas las zonas desde la zona indicada en el input (id de la zona). Genere una visualización de los tiempos de viaje en la zonas con el método plot() de geopandas.

Dada esta explicación, es importante notar que si aplicamos esta función para todas las zonas, tendremos tantas columnas nuevas como número de zonas. Para resolver lo pedido, se dividirá el problema en una serie de miniproblemas o pasos y luego se agruparán cada una de las partes en la función pedida `creator`. Los pasos y explicaciones de estos mismos, son los siguientes.

1. Filtrar la base de datos `tiempos`, de manera tal de solo quedarnos con los que tienen como nodo de origen el id entregado como input a nuestra función.

2. Filtrar la base de datos resultante según el horario recibido como input. **Si bien, esto no se pedía en la M1c, se estimó conveniente agregar el horario como input ya que facilitará los procesos para realizar la Misión 2 de este Lab**. El profesor Francisco Garrido apoyó esta iniciativa mía en el Foro Discord.

- El dato de horario que se recibirá como input será un número entre 0 a 3. El primer tramo horario será cualquier hora pasada las 20:00 y hasta las 6:00 (incluídas), el segundo será cualquier hora pasada las 6:00 hasta las 10:00 (incluída), el tercero cualquier hora pasada las 10:00 y hasta las 17:00 y el último será cualquier hora pasada las 17:00 y hasta (inclusive) las 20:00. Se utilizaron estos tramos, ya que se cree que estos abarcan o agrupan las distintas "horas de taco" que hay en el día.

3. Dado que se presentan horarios distintos por cada par de nodos origen-destino, se debe agrupar esta base de datos bastante filtrada (una especie de "consulta de SQL"), por `ID` para así entregar un solo valor de tiempo por fila, que en este caso será el tiempo promedio de los distintos tiempos promedio por día, de cada valor de `hod`.

4. Fusionamos o *mergeamos* ambas df, procurando que la dataframe elaborada en los pasos anteriores se ajuste a la base de datos `zonas` y no al revés. Es decir, que se mantenga el número de filas de `zonas` (866) en la nueva GeoDataFrame amplificada.

5. Botamos o *dropeamos* las columnas extras del GeoDataFrame resultante del paso 4.

6. Agrupamos todos estos pasos anteriores en una fusión que entrega

Comencemos el largo camino:

Partimos inicialmente estudiando el número de Zonas únicas.
"""

ids = set()
for zona in zonas.ID: 
  ids.add(zona)

print(ids)

print(len(ids))

"""Filtramos el Data Frame de `tiempos` por todas aquellas observaciones que parten en el nodo origen que la función a elaborar pide como input. Se utiliza el ID, 318 a modo de ejemplo."""

id = 318
destinos_id = tiempos.loc[tiempos["sourceid"] == id]
destinos_id.head()

"""Observamos que el método de filtración funcionó bien. 

Luego veamos cuantas observaciones distintas (o horas del día distintas) que hay por el par nodo origen y destino: (318, 311).
"""

destinos_id.loc[destinos_id["dstid"] == 311]

"""Notamos que hay uno por cada hora del día (esto no pasa con todos los nodos destino que provienen del nodo origen 318). Esta tabla anterior, nos servirá para comprobar que los resultados que entrega nuestra función definitiva están bien.

Procedemos luego a calcular la media de cada nodo destino de la DataFrame resultante. Cabe destacar, que nuestra función definitiva filtrará previamente por hora del día y luego agrupará calculando la media. Este paso se omitió, porque no es muy distinto a el filtro que se utilizó para obtener solo las observaciones que tienen como nodo origen el nodo recibido como input.
"""

df2 = destinos_id[["dstid", "mean_travel_time"]].groupby('dstid', as_index = False).mean()

"""Luego "mergeamos" la DataFrame (DF) resultante `df2` con nuestro DF `zonas`. Es importante que la DF resultante se acople a la DF `zonas` y no viceversa. De esta forma nos quedará que para cada uno de los nodos de la tabla `zonas` habrá un nuevo valor correspondiente a la media de los tiempos promedio entre ese nodo y el nodo origen (recibido como input en la función), según el horario entregado, también como input, en la función."""

nueva = zonas.merge(df2, how = "left", left_on='ID', right_on='dstid') 
nueva["tiempo desde {}".format(id)] = nueva["mean_travel_time"]
display(nueva.head())

"""Luego de todo el camino recorrido agrupamos las distintas partes y creamos la función pedida, que se llamará `creator`."""

def creator(zonas, tiempos, id, horario):
  limites = {1: (21,7), 2: (7,11), 3: (11,18), 4: (18,21)}
  desde = limites[horario][0]
  hasta = limites[horario][1]
  #Filtro por nodo origen.
  destinos_id = tiempos.loc[tiempos["sourceid"] == id]
  #Filtro por horario.
  #Como las horas van de 0 a 23, debemos hacer un método especial para el primer horario, que incluye la hora cero.
  if horario == 1:
    destinos_id2 = destinos_id.loc[(destinos_id['hod'] >= desde) | (destinos_id['hod'] < hasta)]
  else:
    destinos_id2 = destinos_id.loc[(destinos_id['hod'] >= desde) & (destinos_id['hod'] < hasta)]
  #Agrupo por nodo destino, para tener un solo tiempo por nodo destino.
  df2 = destinos_id2[["dstid", "mean_travel_time"]].groupby('dstid', as_index = False).mean()
  #"Mergeamos" ambas DataFrames, la creada con la info relevante se acopla al Geodataframe zonas.
  nueva = zonas.merge(df2, how = "left", left_on='ID', right_on='dstid') 
  #Dropeamos columnas extras.
  nueva["Tiempo desde Nodo: {}".format(id)] = nueva["mean_travel_time"]
  nueva.drop("mean_travel_time", axis = 1, inplace = True)
  nueva.drop("dstid", axis = 1, inplace = True)
  #Listo
  return nueva

paracolorear = creator(zonas, tiempos, 734, 4)

paracolorear.head()

tablita = paracolorear['Tiempo desde Nodo: 734'].loc[paracolorear['ID'] == 735]
tablita.values[0]

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [15, 10]
plt.rcParams.update({'font.size': 16})
paracolorear.plot(column = "Tiempo desde Nodo: 734", legend= True)

"""Dado que en el gráfico anterior no se aprecia bien la distribución de las zonas. Se hará un nuevo gráfico que tenga como fondo el gráfico representado en la Misión 1A."""

ax = zonas.plot(color = "white", edgecolor = "grey")
paracolorear.plot(ax = ax, column = "Tiempo desde Nodo: 734" , legend = True)
plt.axis('off')
plt.show()

"""De esta forma podemos notar más claramente los distintos nodos de la Data de Uber que dividen Santiago. Los más morados son a los que se puede llegar más rápido, y los más amarillos o verdes, lo que más demoramos en llegar.

Los blancos, son de los que no hay data para ese par nodo-origen / nodo-destino, y para ese momento del día que abarca el horario entregado como input.

## Misión 2: Velocidades a Nivel de Red.

En esta misión deberá añadir información de velocidades a los arcos de la red vial de Santiago para distintos tramos horarios. Para llevar a cabo la misión, guíese con los siguientes pasos:

### Misión 2.A:

Utilice el archivo Shape de las zonas urbanas de Chile para obtener la zona urbana de Santiago. A partir de la zona urbana obtenida, obtenga mediante la librería de `osmnx` la vialidad contenida dentro del polígono la zona urbana.

Primero visualizamos el área urbana de Santiago de Chile. Luego debemos obtener las vialidades contenidas dentro de este polígono, graficado más abajo de color verde.
"""

df_urbano = gpd.read_file('areas_urbanas.shp')

df_urbano.head()

df_urbano.loc[df_urbano["NOMBRE"] == "Santiago"].plot(color = "forestgreen")

"""Para conocer mejor los datos de este archivo, debemos estudiar el código EPSG de zona_urbana."""

df_urbano.crs

"""Notamos que este tiene un código de EPSG de **32719**. Esto significa que difiere del código que usa Google Maps, por ende si queremos hacer joins con información de esta tabla con la Geodataframe `zonas`, debemos antes modificar las coordenadas utilizadas. Como queremos obtener las vialidades dentro de esta zona urbana, y la librería OSMNX trabaja con las mismas coordenadas de Google Maps (EPSG: 4326), convertimos al tiro las geometrías a esta forma.


"""

df_urbano2 = df_urbano.to_crs(4326)

figura = []

for num in df_urbano2.loc[df_urbano["NOMBRE"] == "Santiago"]["geometry"]:
  figura.append(num)

print(type(figura[0]))

poligono = figura[0]

"""En lo siguiente se procederá a encontrar las vialidades contenidas dentro del área urbana de Santiago. Sin embargo antes, visualizaremos las vialidades de la comuna de Las Condes, para apreciar lo poderosa que es esta librería."""

import networkx as nx
import osmnx as ox

ox.plot_graph(ox.graph_from_place('Las Condes, Chile'), figsize = (20,20), bgcolor = 'w', node_color = 'red', edge_color = 'black', node_alpha = 0)

"""Habiendo visto, la representación de las vialidades de la comuna de Las Condes, procedemos a extraer todas las vialidades que estén dentro de la zona urbana de Santiago. Para ello se usa el método de `osmnx`, llamado *graph_from_polygon*. A este le entregamos el multipolígono, que es la geometría que envuelve el área urbana de la ciudad de Santiago, y le pedimos que guarde en el objeto `vialidades_urbanas`. Este objeto es un multidigraph. """

vialidades_urbanas = ox.graph_from_polygon(poligono, network_type= 'drive')

type(vialidades_urbanas)

"""Para graficar este multidigraph creado, debemos seguir los siguientes pasos. Cabe mencionar, que no se denotaron en el gráfico los nodos. Esto se omitió ya que al añadirlos a la visualización, no se alcanzaban a apreciar las miles de vialidades que contiene nuestra capital."""

vu_projected = ox.project_graph(vialidades_urbanas)

ox.plot_graph(vu_projected, figsize = (30,30), bgcolor = 'w', node_color = 'red', node_size=3, edge_color = 'black')

"""### Misión 2.B:

Defina una metodología propia que permita incorporar a la vialidad las velocidades presentes en arcos de la red vial. Las velocidades deben ser obtenidas a partir de la información de tiempos de viaje generada en la M1. Debe explicar detalladamente la lógica utilizada y justificarla para que tenga sentido. Puede ayudarse con los métodos disponibles en la librería de osmnx, geopandas o cualquier otra librería que encuentre útil (referenciando cuando corresponda). Finalizada esta parte, deberá tener un Shape de la red vial en donde donde cada geometría de vialidad (LineString) incluya información de la velocidad para distintos horarios del día. Ustedes es libre de definir el nivel de agregación temporal en base a los datos, siempre y cuando tenga al menos cuatro horarios diferentes. Genere una visualización con esquema de color de las velocidades para cuatro horarios diferentes.

En esta misión hubo un trabajo previo muy arduo, de "craneación" de los pasos a seguir. La dificultad radica en que para obtener la velocidad promedio de una vía se requiere la distancia de la vialidad y también el tiempo que se demora en recorrerla en ese momento del día. 

La distancia la podemos obtener sin problemas de la librería Open Street Map. Sin embargo, el tiempo no es tan fácil conseguirlo. En la M1, solo tenemos tiempos de viaje entre una zona y otra. Pero es de esperar que tengamos vías que están dentro de una misma zona, por lo que si usamos estos tiempos, moverse dentro de una zona tendrá tiempo indefinido. Además no sabemos como podría distribuirse el tiempo entre las vías que se usan para llegar a otra zona desde una zona en específico. 

Se procederá entonces a probar con distintas formas. Inicialmente se consideró útil estudiar la funcionalidad del módulo de Open Street Map: **add_edge_speed**. Se procederá a agregar a nuestro multi-di-graph las velocidades de cada edge, según la velocidad permitida para recorrer esa vía urbana. Esta "nueva columna" del GeoDataFrame, se utilizará más adelante.

Para resolver este desafío se consideró oportuno hacer el siguiente proceso, para cada par de zonas aledañas:

1. Seleccionar aleatoriamente **x** nodos de una zona y **x** de su zona vecina (para ello hay que definir previamente, cuales son las zonas vecinas).

2. Habiendo seleccionado estos nodos, se procede a calcular el shortest path (con weight = `travel_time`, el camino más RÁPIDO según OSMNX) entre cada una de las posibles combinaciones de nodos entre la zona y su zona vecina. Teniendo cada una de las **y** rutas posibles, se tomará una ruta, y luego calcularé el tiempo promedio demorado en cada uno de los arcos (usando la velocidad entregada por Open Street Map y la distancia de cada arco). Para terminar este paso, se sumará el tiempo demorado en recorrer cada uno de los arcos que compone esta ruta. Repetimos este proceso para cada unas de las **y** rutas obtenidas en el paso 2. 
- En vez de hacer lo anterior tal cual, se utilizó el método de Open Street Map: `shortest_path_length`, con el weight `travel time`. De esta forma obtenemos el tiempo total demorado en la ruta completa. Emulando lo que se quería hacer en este paso 3. Teniendo en mano el tiempo demorado en cada una de las rutas, se calcula el tiempo promedio total, demorado en cada una de las **y** rutas.

3. Se divide el tiempo promedio que Uber dice que nos demoramos en cruzar de esa zona a su vecina específica, en un cierto momento del día.

4. El cuociente obtenido en 5, se utiliza para multiplicar cada una de las velocidades de los arcos obtenidas con el método de open street map `add_egde_speed`.

5. Si hay arcos que son compartidos por distintas rutas, no nos complicaremos y dejaremos cualquiera de las dos velocidades, obtenidas en el paso 6.

6. Los arcos que no forman parte de alguna de estas **y** rutas (*arcos repudiados*), llevarán la velocidad entregada por open street map, sin importar el momento del día. El número de arcos repudiados, decrece con un mayor nivel de x. Lamentablemente mi computador no es tan bueno, así que utilizaré un número no grande de **x**. Pero los invito, a usar un número mayor en sus computadores más *pros*.

#### Agregando Velocidades de Open Street Map a las vialidades.

Primero guardamos el  Multi-DiGraph obtenido en la Misión 2A, de manera tal de tener la información de sus nodos y de sus arcos (vialidades) en un GeoDataFrame. Habiendo realizado esto, se hacer rápidas visualizaciones de los DF obtenidos para entender mejor su composición y lograr una mayor familiarización con los datos.
"""

vialidades_urbanas2 = ox.add_edge_speeds(vialidades_urbanas, precision=1)

vialidades_urbanas3 = ox.add_edge_travel_times(vialidades_urbanas, precision=2)

ox.save_graph_shapefile(vialidades_urbanas3)

ls

nodos = gpd.read_file('data/graph_shapefile/nodes.shp')

arcos = gpd.read_file('data/graph_shapefile/edges.shp')

"""**Visualización de los nodos:**"""

nodos.head()

nodos.shape

nodos['highway'].value_counts()

"""**Visualización de las Vialidades (con sus respectivas velocidades):**"""

arcos.tail()

arcos['highway'].value_counts()

arcos.shape

import numpy as np
arcos['speed_kph'].describe()

"""#### 1. Seleccinando Nodos Aleatorios:

Se procederá a definir una función que seleccione nodos aleatorios de cada zona. Para obtener esta función se necesita primero *linkear* nuestro nuevo GeoDataFrame de nodos (compuesto por puntos) a nuestro GeoDF de `zonas`, así podemos saber a que zona pertenece cada nodo.

Además se debe definir que es una zona vecina.

Partamos por lo primero:
"""

ax = zonas.plot(color = "white", edgecolor = "grey")
nodos.plot(ax = ax, color = "red", markersize= 0.1, marker = 'x')
plt.axis('off')
plt.show()

"""Para agilizar el proceso de encontrar la zona correspondiente a cada nodo, se filtrarán las zonas que están dentro de Santiago Urbano."""

zonas2 = gpd.sjoin(zonas, nodos, op='intersects', how = 'inner')

zonas2.head()

revisar = zonas2[['ID','geometry']]

revisar = revisar.drop_duplicates()

"""En la siguiente gráfica podemos ver como queda el mapa que contiene las zonas que tocan en algún punto algún nodo de alguna vialidad correspondiente a Santiago Urbano."""

ax = revisar.plot(color = "yellow", edgecolor = "lightgrey")
nodos.plot(ax = ax, color = "blue", markersize= 0.1, marker = 'x')
plt.axis('off')
plt.show()

"""Ahora debo crear una función del tipo `lambda` que reciba un nodo y te devuelva la zona. Para esto se utilizó información de la siguiente página web: https://automating-gis-processes.github.io/CSC18/lessons/L4/point-in-polygon.html#point-in-polygon-using-geopandas

"""

nodos.loc[0, 'geometry'].within(zonas.loc[0, 'geometry'])

diccio = {}
for i in range(len(revisar)):
  diccio[revisar.iloc[i, 0]] = revisar.iloc[i, 1]

print(diccio)

def encontrar_zona(nodo, dfzonas = diccio):
  for id in dfzonas.keys():
    if nodo.touches(dfzonas[id]) or nodo.within(dfzonas[id]):
      return int(id)

encontrar_zona(nodos.loc[0, 'geometry'])

nodos['zonas'] = nodos['geometry'].apply(lambda x : encontrar_zona(x))

nodos.tail()

#Graficar nodo con su zona

ax = zonas.plot(color = "white", edgecolor = "grey")
nodos.plot(ax = ax, color = "red", markersize= 0.1, marker = 'x')
plt.axis('off')
plt.show()

nodos.apply(lambda x: sum(x.isnull()),axis=0)

nodos.shape

#nodos.drop("zonas", axis = 1, inplace = True)

"""Notamos que ahora tenemos que para casi cada nodo, este tiene su ID de zona respectiva.  Esto nos permitirá más adelante elegir por cada zona un número **x** de nodos aleatorios. Lamentablemente hay 340 nodos, a los cuales no se les asignó ninguna zona. Esto, la verdad es que no logré entender por qué pasaba, dado que se consideró que un nodo pertenece a una zona es específico, si está dentro de el o si está sobre su frontera. El considerar todas las zonas, y no solo las filtradas de `revisar`, tampoco ayudó. Por todo esto, se hizo caso omiso de estos valores faltantes que corresponden al 0.0035% de los nodos originales.

Si bien tengo claro que la idea era no usar tanto Python. Lamentablmente, los múltiples códigos que ideé se centraban más en funciones del tipo lambda, no eran tan eficientes (y demoraban infinito en ejecutarse), así que preferí implementar la función `encontrar_zona` como se encuentra más arriba.

Ahora procederemos a definir rápidamente lo que es una zona vecina de otra zona.

Con el siguiente for loop, pudimos obtener los vecions de cada una de las zonas, y así tenemos una nueva columna `vecinos` que contiene para cada zona, una lista con sus respectivos vecinos. Esta idea de código fue sacada de: https://gis.stackexchange.com/questions/281652/find-all-neighbors-using-geopandas/281676

Al código se le implementaron una serie de `try` y `except` para poder lidiar con las excepciones levantadas, al estudiar que polígonos tocan o colindan con otros.
"""

#zonas["vecinos"] = None 

#for index, row in zonas.iterrows():
    #try:
      #vecinos = zonas[zonas.geometry.touches(row['geometry'])].ID.tolist()
    #except:
      #pass 
    #try: 
      #vecinos = vecinos.remove(row.ID)
    #except:
      #pass
    #zonas.at[index, "vecinos"] = vecinos

zonas["vecinos"] = None 

for index, row in zonas.iterrows():
    try:
      vecinos = np.array(zonas[zonas.geometry.touches(row['geometry'])].ID)
    except:
      pass
    try:
      overlap = np.array(zonas[zonas.geometry.overlaps(row['geometry'])].ID)
    except:
      pass
    vecinos = np.union1d(vecinos, overlap) 
    try: 
      vecinos = vecinos.remove(row.ID)
    except:
      pass
    zonas.at[index, "vecinos"] = vecinos

"""Habiendo añadido los vecinos, estudiamos si estos fueron bien implementados a simple vista. Vemos la comuna de Santiago y al parecer los vecinos fueron seleccionados de buena manera. Estoy consiente que quizás algunos vecinos no fueron encontrados perfectamente, pero como este estudio es un estudio *juvenil*, espero que hagan vista gorda jaja."""

zonas.loc[zonas['Comuna'] == 'Vitacura']

zonas.loc[zonas['Comuna'] == 'Vitacura'].plot(column = 'ID', legend = True)

ax = revisar.plot(color = "tomato", edgecolor = "lightgrey")
zonas.loc[zonas['Comuna'] == 'Vitacura'].plot(ax = ax, column = 'ID', legend = True)
plt.axis('off')
plt.show()

"""
El procedimiento para seleccionar aleatoriamente nodos, de una zona en específico, se implementará al agrupar cada uno de los pasos. Pero la idea es extrer una serie de números aleatorios, dentro del intervalo que va de cero hasta el número de nodos de esa zona es específico y luego extraer los nodos seleccionados con el método `.iloc()`. Estos nodos seleccionados junto a los seleccionados de la otra zona, serán a los que se les calculará el camino mínimo entre ellos más adelante.

El código se verá de la siguiente manera:"""

#nodos['zonas'] = nodos['zonas'].apply(lambda x : try int(x))

def numerizar(elem):
  try:
    num = int(elem)
    return num
  except:
    pass

nodos['zonas'] = nodos['zonas'].apply(lambda x : numerizar(x))

nodos.head()

nodos.loc[nodos['zonas'] == 318]

import random

def seleccion_nodos(id_zona, nodos, x):
  '''Recibe un ID de una zona y retorna x nodos seleccionados aleatoriamente'''
  df = nodos.loc[nodos['zonas'] == id_zona]
  if x > len(df):
    x = len(df)
  lista = list()
  while len(lista) != x:
    num = random.randint(0, len(df)-1)
    if num not in lista:
      lista.append(num)
  seleccion = []
  for num in lista:
    seleccion.append(df.iloc[num, 2])

  return seleccion

seleccion_nodos(318, nodos, 2)

"""De esta forma, obtenemos el osmid de 5 nodos distintos, de manera tal que ya podemos encontrar el shortest path entre todas las posibles combinaciones entre los **x** nodos seleccionados de esta zona y una de sus zonas vecinas.

Entonces se aplicará esta función para la zona, y luego para cada uno de sus vecinos.

#### 2. Shortest Path entre nodos.

Luego de haber seleccionado los **x** nodos de manera aleatoria de dos zonas aledañas o vecinas. Procederemos a encontrar el camino más rápido entre ambos nodos. Para ello crearemos una función que encuentre el camino más rápido para cada posible combinación de estos nodos. 
Para encontrar cada posible combinación (sin repetición) usaremos la librería `itertools` en especial su paquete de `combinations`. 

Entonces para una zona en específico, seleccionaremos una de sus vecinas (que se encuentra en la columna `vecinos` del  GeoDF `zonas`) y luego obtendremos los **x** nodos aleatorios de ambas zonas.

Para seleccionar los vecinos de una zona en específico debemos hacer lo siguiente:
"""

for i in range(4):
  print('ESTE ES EL ID: ' + str(zonas.loc[i, "ID"]))
  for vecino in zonas.loc[i,"vecinos"]:
    print(vecino)

"""Notamos que de esta forma se puede seleccionar cada uno de los vecinos de una zona es específico (usando el segundo for loop).

Procedemos ahora a señalar como se calcula el camino, o ruta más corta, entre dos nodos. Para ello primero debemos entender bien como se puede lograr calcular la ruta más corta con la estructura de GeoDF y MultiDiGraph´s que tenemos hasta el momento.
"""

nodos[nodos['zonas'] == 318].head()

orig_node = seleccion_nodos(318, nodos, 1)
osmid1 = orig_node[0]
osmid1

"""Procedemos a hacer lo mismo con la zona vecina:"""

dest_node = seleccion_nodos(301, nodos, 1)
osmid2 = dest_node[0]
osmid2

route = nx.shortest_path(vialidades_urbanas, osmid1, osmid2, weight = 'travel_time')

route

arcos.loc[(arcos['v'] == 6214163543) & (arcos['u'] == 250439348)]

"""Con el siguiente código podemos saber cuanto se demora a través de la ruta más corta. Este código permitirá"""

travel_time = nx.shortest_path_length(vialidades_urbanas, osmid1, osmid2, weight='travel_time')
round(travel_time)

"""Veamos que efectivamente se haya calculado una ruta creíble y realista. Notamos que los nodos fueron seleccionados entre dos zonas vecinas y que la ruta al parecer es verdaderamente la más corta."""

fig, ax = ox.plot_graph_route(vialidades_urbanas, route, node_size = 0.1, figsize=(20,20), bgcolor = 'w', node_color = 'blue', edge_color = 'black')

"""En lo siguiente aprovecharemos de agregar a nuestro GeoDataFrame de arcos el tiempo que se demora en recorrer este arco según las velocidades entregadas por osmnx.

Agrupemos todo lo anterior en una sola función que obtenga la ruta entre dos nodos.
"""

from itertools import product

a = [1, 2]
b = [13, 12]
c = list(product(a, b))

c

"""La siguiente función `time_caminosmascortos` recibe dos zonas aledañas y calcula el tiempo promedio demorado en cruzar estas zonas. Para ello selecciona x nodos aleatorios de cada zona, calcula el tiempo total demorado entre un nodo a otro, siguiendo el camino más corto (**se supone que los conductores de Uber siguen la ruta más corta de un punto a otro [supuesto bastante realista]**). Esta función retorna el tiempo promedio entre cada una de las rutas posibles entre cada par de nodos origen (zona) y destino (vecina)."""

def time_caminosmascortos(nodos, zona, vecina, vialidades_urbanas, x):
  '''Recibe dos zonas aledañas, calcula el tiempo promedio demorado 
  en cruzar estas zonas.'''
  nodos0 = seleccion_nodos(zona, nodos, x)
  nodos9 = seleccion_nodos(vecina, nodos, x)
  combi = list(product(nodos0, nodos9))
  tiempo_total = 0
  for elem in combi:
    osmidA = elem[0]
    osmidB = elem[1]
    try:
      travel_time = nx.shortest_path_length(vialidades_urbanas, osmidA, osmidB, weight='travel_time')
    except:
      pass
    tiempo_total += round(travel_time)
  if len(combi) != 0:
    return tiempo_total/len(combi)
  else:
    return np.nan

time_caminosmascortos(nodos, 318, 301, vialidades_urbanas, 2)

type(np.nan)

"""#### 3. Obteniendo la Fracción Clave:

Con esta *fracción clave*, podremos más adelante obtener la velocidad que demora a una cierta hora del día. Lo que se hace en este paso es bastante simple.

Según el tiempo demorado desde una zona específica hacia su vecina en un cierto momento del día (obtenidos en la misión 1), podremos obtener las 4 fracciones claves, que multiplicarán las velocidades de OSMNX para obtener las velocidades en cada uno de esos 4 posibles horarios del día, según los tiempos promedio de viaje de UBER (de días de semana).
"""

def fraccion_clave(nodos, zona, vecina, horario, zonas, tiempos, vialidades_urbanas, x):
  denominador = time_caminosmascortos(nodos, zona, vecina, vialidades_urbanas, x)
  #Para obtener denominador camino es un poco más largo.
  df = creator(zonas, tiempos, zona, horario)
  tablita = df['Tiempo desde Nodo: {}'.format(zona)].loc[paracolorear['ID'] == vecina]
  numerador = tablita.values[0]
  if denominador and numerador != np.nan:
    return (numerador / denominador)

fraccion_clave(nodos, 318, 301, 4, zonas, tiempos, vialidades_urbanas, 100)

fraccion_clave(nodos, 734, 735, 1, zonas, tiempos, vialidades_urbanas, 100)

fraccion_clave(nodos, 674, 684, 1, zonas, tiempos, vialidades_urbanas, 10)

"""#### 4. Cuociente Clave por Velocidades Arcos de todos los Shortest Path del par de zonas:

Para poder llevar la fracción clave a el GeoDataFrame compuesto por los arcos, debemos agarrar la velocidad de cada uno de los arcos que formaban parte de alguna de las rutas calculadas en el paso 2, y multiplicarla por nuestra fracción clave. Para ello es importante tener en cuenta que no podemos repetir el proceso de selección de nodos, sino multiplicaríamos nuestra fracción clave a arcos que no corresponden ser amplificados o disminuidos.

Crearemos una función `final` que permita hacer este proceso y modificar el GeoDataFrame de los arcos, con las nuevas velocidades según los tiempos calculador por Uber. Para ser consistente con la teoría propuesta, no usaremos las mismas funciones creadas en los pasos anteriores, pero si la estructura será exactamente la misma.

Esta función `final` recibe una zona, una zona aledaña o vecina, un valor de x (número de nodos a 
"""

def final(nodos, zona, vecina, horario, zonas, tiempos, vialidades_urbanas, x, arcos):
  
  #Selecciono Nodos.
  nodos0 = seleccion_nodos(zona, nodos, x)
  nodos9 = seleccion_nodos(vecina, nodos, x)
  
  #Obtengo todas las posibles combinaciones entre ambos grupos de nodos.
  combi = list(product(nodos0, nodos9))
  tiempo_total = 0
  #Listado de los nodos origen destino que agrupan los arcos.
  rutas = []
  
  for elem in combi:
    osmidA = elem[0]
    osmidB = elem[1]
    try:
      route = nx.shortest_path(vialidades_urbanas, osmidA, osmidB, weight = 'travel_time')
      rutas.append(route)
    except:
      pass
    try:
      travel_time = nx.shortest_path_length(vialidades_urbanas, osmidA, osmidB, weight='travel_time')
      tiempo_total += round(travel_time)
    except:
      tiempo_total = np.nan
  
  if len(combi) != 0:
    denominador = tiempo_total/len(combi)
  else:
    denominador = np.nan
  
  #Obtengo denominador para frac_clave
  df = creator(zonas, tiempos, zona, horario)
  tablita = df['Tiempo desde Nodo: {}'.format(zona)].loc[paracolorear['ID'] == vecina]
  numerador = tablita.values[0]
  if denominador and numerador != np.nan:
    frac_clave = (numerador / denominador)
    
  #Multiplico uno por uno los arcos a modificar velocidad.  
  for camino in rutas:
    dfarcos = arcos.copy()    
    for i in range(len(camino)-1):
      alejito = dfarcos.loc[((dfarcos['v'] == camino[i]) & (dfarcos['u'] == camino[i+1])), 'speed_kph']
      try:
        indice = alejito.index[0]
        dfarcos.at[indice, 'speed_kph'] *= frac_clave
        return dfarcos
      except:
        dfarcos = arcos.copy()
        return dfarcos

arcosZ = final(nodos, 318, 301, 4, zonas, tiempos, vialidades_urbanas, 2, arcos)

"""Revisemos si la velocidad cambió (ya revisé que cambiaba según fracción clave). Efectivamente la velocidad cambió de manera correcta. Si bien, esta no es muy realista, no pretendía con mi algoritmo hacer una representación idéntica a la realidad. 

Creo sin embargo, que el algoritmo con un x lo suficientemente grande, puede converger asintóticamente al óptimo (si no me equivoco).
"""

arcos.loc[((arcos['v'] == 6214163543) & (arcos['u'] == 250439348))]

arcosZ.loc[((arcosZ['v'] == 6214163543) & (arcosZ['u'] == 250439348))]

"""#### 5. Aplicando a todo el GeoDataFrame de Arcos nuestro algoritmo.

Para ello se deben recorrer todas las zonas, y cada uno de sus vecinos. Si el arco que se está recorriendo ya se le modificó la velocidad, esta se modificará nuevamente. Pese a que se puede arreglar facilmente esto, se prefirió dejar así para que la intuición del algoritmo usado no sea más enredado todavía (pero basta con crear una columna en `arcos` que tome el valor de 1 si fue modificada o 0 si no, si el arco ya fue modificado [tiene en la nueva columna valor 1] no hay que modificarlo nuevamente. 

Utilizamos solo dos nodos seleccionados aleatoriamente (**x** = 2) dado que si no el computador tomará mucho tiempo en hacer lo pedido. Lo ideal es utilizar un **x** lo más grande posible.

En este proceso de ir añadiendo las velocidades a los arcos, hay que tener presente que debemos ir actualizando nuestra base de datos constantemente.
"""

cosa = zonas.loc[zonas["ID"] == 1, 'vecinos'].values
cosa[0][4]

def velocidad(x, nodos, zonas, tiempos, vialidades_urbanas, ids, arcos, horario):
  df = arcos.copy()
  for id in ids:
    vecinos = zonas.loc[zonas["ID"] == id, 'vecinos'].values
    vecinos2 = vecinos[0]
    for vecino in vecinos2:
      df = final(nodos, id, vecino, horario, zonas, tiempos, vialidades_urbanas, x, arcos)
  return df

arcos_final_horario1 = velocidad(2, nodos, zonas, tiempos, vialidades_urbanas, ids, arcos, 1)

arcos_final_horario2 = velocidad(2, nodos, zonas, tiempos, vialidades_urbanas, ids, arcos, 2)

arcos_final_horario3 = velocidad(2, nodos, zonas, tiempos, vialidades_urbanas, ids, arcos, 3)

arcos_final_horario4 = velocidad(2, nodos, zonas, tiempos, vialidades_urbanas, ids, arcos, 4)

"""Lamentablemente, no fui capaz de modificar el GeoDataFrame completo. Muchas excepciones se levantaron a medida que trataba de aplicar mi lógica. Estas se daban por la generación de nodos aleatorios de dos zonas, que no podían ser conectados entre sí. También tuve un problema que no se por qué al convertir el MultiDiGraph de `vialidades_urbanas` a un GeoDataFrame, algunos arcos no fueron incluidos en el GeoDataFrame, por lo que al buscar la información de los arcos que conectaban los nodos entregados por el algoritmo de `shortest_path`, me respondía que estos arcos no existían. Estos  problemas y miles de otros que me topaba en esta larga (pero ultra bonita) travesía, significaron mucho tiempo (bien invertido, pero en fin) que por ser fin de semestre no pude resolver antes de la entrega. Si bien, partí desarrollando este lab con mucha anticipación, lamento no haber podido llegar a la solución final, pero espero que al menos reciba un puntito por este camino tan tortuoso que implicó este desarrollo del cual estoy MUY pero MUY orgulloso. Muchas gracias!

#### Visualización:

Sin embargo, para no quedarme con las manos vacías para la visualización, hice una visualización con las velocidades entregadas por osmnx, cuando uno llama el método `add_edge_speed`.

Para desarrollar esta visualización quise agrupar las zonas por velocidad promedio, y así imprimir las zonas más veloces y las más lentas. Para ello fue necesario asignar a cada uno de los arcos de nuestro GeoDF `arcos`, su zona correspondiente. Para ello se utilizó como criterio que si el nodo donde parte el arco (**from** o **v**), esta ubicado en una zona **z**, entonces el arco pertenece a la zona **z**.

Para lograr esto se usará algo del estilo apply que agarre el osmid del nodo donde parte el arco, y rellene la nueva columna de `arcos` con la zona que le corresponde a ese mismo nodo. La lógica utilizada es similar a la que usé para asignar a cada nodo una zona, solo que ahora no se utiliza ningún for.

Cremos un data frame, que solo tenga los datos de `osmid` y `zonas` de el GeoDF de `nodos`.
"""

dfnodoszona = nodos[['osmid','zonas']]
dfnodoszona.head()

"""Creamos una función que para cada valor de `from` de un arco que reciba, entregue el valor que le corresponde de zona."""

def encontrar_zona2(arco, dfzonas = dfnodoszona):
  return dfzonas.loc[dfzonas['osmid'] == arco, 'zonas'].values[0]

encontrar_zona2(arcos.loc[0, 'from'])

"""Aplicamos nuestra función que funciona bien, como se puede ver en el cuadro anterior y en la visualización de `arcos``
 modificado.
"""

arcos['zonas'] = arcos['from'].apply(lambda x: encontrar_zona2(x))

arcos.head()

"""Ahora agrupamos por zona según la media de la velocidad de cada uno de sus arcos, usando el método `group_by` de la librería pandas."""

paravisualizar = arcos.groupby(['zonas']).mean()
paravisualizar2 = paravisualizar['speed_kph']
paravisualizar2.head()

"""Con lo anterior, obtenemos la media de cada una de las columnas numéricas de nuestro GeoDF `arcos`, agrupados por el valor de zona. De este nuevo GeoDF solo nos interesa la columna `speed_kph`, la velocidad promedio por zona.

Agregamos esa columna a nuestro GeoDF de `zonas`. Pero para no tener enredos, creamos un `zonas3` que contenga esta nueva variable. Es necesario hacer esto porque para visualizar es solo un DF, no un GeoDF.

Para ello hacemos algo parecido a lo que hicimos para agregar a cada arco una zona en específico.
"""

zonas3 = zonas.set_index("ID")

zonas3.head()

listoco = pd.concat([zonas3, paravisualizar2], axis = 1)

listoco.head()

listoco.plot(column= 'speed_kph', legend = True)

"""Para tener mayor variedad de color, aplicamos log a la columna de `speed_kph` de el GeoDF `listoco`."""

listoco['speed_kph'] = np.log(listoco['speed_kph'])

listoco.plot(column= 'speed_kph', legend = True)

"""Algo mejora la variedad de colores.

De haber tenido bien mi función `velocidad`, hubiese hecho este mismo proceso para cada una de los 4 GeoDF, del tipo `arcos_final_horario1`, `arcos_final_horario2`, `arcos_final_horario3`, `arcos_final_horario4`. Luego hubiese tenido las 4 visualizaciones pedidas. Para automatizar este proceso de manera más elegante, hubiese creado la función plotear, que agrupase cada uno de estos pasos, y así aplicar a cada uno de los 4 GeoDFs mencionados una sola función, y en una sola linea obtener el código pedido. La función `plotear` se encuentra en el siguiente cuadro:
"""

def plotear(nodos, arcos_final_horarioH):
  dfnodoszona = nodos[['osmid','zonas']]
  arcos['zonas'] = arcos['from'].apply(lambda x: encontrar_zona2(x))
  paravisualizar = arcos.groupby(['zonas']).mean()
  paravisualizar2 = paravisualizar['speed_kph']
  zonas3 = zonas.set_index("ID")
  listoco = pd.concat([zonas3, paravisualizar2], axis = 1)
  listoco['speed_kph'] = np.log(listoco['speed_kph'])
  return listoco.plot(column= 'speed_kph', legend = True)

plotear(nodos, arcos)

"""Quizás lo que se buscaba con la visualización, era que por cada uno de los 4 horarios hacer una visualización de los arcos y sus velocidades respectivas, más que la velocidad promedio de los arcos de cada zona. Si esto último es cierto, hubiésemos tenido que correr para cada uno de los GeoDFs armados con la función `velocidad` (`arcos_final_horario1`, `arcos_final_horario2`, `arcos_final_horario3` y `arcos_final_horario4`) la siguiente línea de código."""

arcos.plot(column = 'speed_kph', figsize = (20,20), legend = True)

"""**IMPORTANTE**

Notamos que la velocidad de ciertas calles es bastante más alta, y alguna bastante baja. **Si tuviésemos la velocidad de cada horario, con la información de UBER, podríamos comparar hacernos una idea de cuanto demoran al trabajo (en los días de semana, por eso usamos solo esos tiempos de viaje) las personas según donde viven. Para ello debiéramos tener especial ojo con los horarios 2 y 4. Siendo estos últimos correspondientes a el horario de ida al trabajo y el de vuelta respectivamente.** 

Quizás podríamos tener más variedad de color si aplicásemos log a la columna `speed_kph`, veamos que pasa.
"""

arcosV = arcos.copy()
arcosV['speed_kph'] = np.log(arcosV['speed_kph'])
arcosV.plot(column = 'speed_kph', figsize = (20,20), legend = True)

"""Notamos que no cambia mucho, pero igualmente es bonito de tenerlo en este informe, así que con esto cerramos la fallida Misión 2b.

## Misión 3: 

En esta misión deberá buscar por internet algún tipo de información que pueda añadir a sus datos (que sea geo-referenciable de alguna forma). Para completar esta misiión deberá hacer uso de web scrapping para obtener la información de internet. Ayúdese con el tutorial de expresiones regulares (disponible en el Syllabus) para procesar los datos obtenidos. Genere una visualización de la ciudad de Santiago que incluya esta característica. Es libre de generar la visualización de mapa que desee siempre y cuando incluya relación con la característica obtenida.

En la siguiente sección se extraerá información sobre los resultados en el Plebiscito Constitucional de Octubre del 2020. La página para extraer esta información es un famoso medio de comunicaciones deportivo, que hoy en día comunica todo tipo de noticias. El url específico a utilizar es el siguiente: https://chile.as.com/chile/2020/10/26/actualidad/1603716098_944619.html.

La idea es hacer con esto relaciones entre como fue el porcentaje de victoria del Apruebo en cada comuna, y luego relacionar esta variable con los tiempos de traslado (necesario para la misión 4). La verdad es que no se bien que esperar de la relación entre tiempos de viaje y resultado en el plebiscito, por eso mismo me parece interesante estudiar la relación entre ambas variables.

Siguiendo la línea de la materia de clases, debemos primero desargar el código fuente de la página.

Antes sin embargo, debemos instalar las siguientes librerías:

#### Instalación de Librerías:
"""

!pip3 install pyrematch

"""#### Parte Web-Scrapping: 
Ahora seguimos con la extracción de los porcentajes de victoria del apruebo y el nombre de la respectiva comuna.
"""

import urllib.request as net
import ssl

class WebDownloader:
    
    def __init__(self, link):
        self.user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'
        self.url =  link
        
        
    def getHtmlAsString(self):
        headers = {'User-Agent':self.user_agent}
        request= net.Request(self.url,None,headers)
        gcontext = ssl.SSLContext()
        response = net.urlopen(request,context=gcontext)
        return response.read().decode('utf-8')
    
wd = WebDownloader('https://chile.as.com/chile/2020/10/26/actualidad/1603716098_944619.html')
sourceCode = wd.getHtmlAsString()
#print(sourceCode)

"""Notamos que los datos de los resultados se encuentran dentro de categorías del tipo `p` (saqué la impresión del `sourceCode`, para no ser latero). Por lo tanto, con el método de la librería `beautiful soup 4`, `find_all`, extraeremos todos los datos que estén dentro de este tipo de categoría. Además notamos que es necesario limpiar cada una de las observaciones que nos quedan en nuestra lista `info`, ya que siempre parten con una doble comillas y una *b*, y son cerradas con doble comillas al final. De esta forma, será más fácil luego filtrar la información que queremos usando la buenísima librería `pyrematch` vía la utilización de expresiones regulares."""

import bs4
info = []
soup = bs4.BeautifulSoup(sourceCode)
for node in soup.findAll('p'):
    info.append(str(u''.join(node.findAll(text=True)).encode('utf-8'))[2:-1])

print(info)

"""Al igual que para la actividad práctica 05, se utilizó constantemente la página web: www.regex101.com sugerida por Pablo en la última ayudantía del curso.

A través de esta página, no se hizo tan latero el proceso para filtrar los datos que quería extraer. En particular estos son los datos del nombre de la comuna y los datos del porcentaje de victoria del apruebo.

Partamos extrayendo el porcentaje de victoria del apruebo.
"""

import pyrematch as re

seq = info
pattern = "(Apruebo: )!porcentaje{.....}(%)"
regex = re.compile(pattern)

apruebo  = []
for s in seq:
  for match in regex.finditer(s):
    apruebo.append(match.group('porcentaje'))

print(apruebo)

"""Aprovechamos de convertir a número cada uno de estos porcentajes."""

aprue = []
for porcentaje in apruebo:
  porcentaje = porcentaje.replace(',','.')
  aprue.append(float(porcentaje))

print(aprue)
apruebo = aprue

"""Ahora procedamos a extraer el nombre de cada una de las comunas. Se pensó utilizar un pattern como el siguiente:

`pattern = "^[a-zA-ZÑñ\s]*$"`

Pero no resultó, dado que teníamos algunos elementos de la lista `info`, que cumplían este criterio. El problema fue entonces que no tenemos un criterio único para filtrar las comunas. Por ello se limpió inicialmente nuestra lista `info` de los primeros 6 elementos de la lista y de los últimos 4 elementos. 
"""

info2 = info[6:-4]
print(info2)

"""Sin embargo, luego usamos un pattern que nos aseguraba de extraer todas las comunas, sin importar que estas tuviesen ñ o no y sin importar que tuviesen tilde o no. Curiosamente no funcionó bien el encoding, quizás no funciona bien a la hora de hacer prints, pero no logré pillar en ningún lado por qué ocurría, este *mal funcionamiento*.

El pattern utilizado, fue uno que permitía seleccionar todas las observaciones de nuestra lista info que no tuviesen %. El pattern es:

`pattern = "(%)"`

De esta forma, obtuvimos un set llamado `nocomunas` que tenía todas las observaciones de `info2` que no correspondían a comunas (basicamente contenía los NO múltiplos de 3).
"""

seq2 = info2
pattern = "(%)"
regex = re.compile(pattern)

nocomunas = set()
for i in range(len(seq2)):
    if regex.find(seq2[i]):
      nocomunas.add(i)

print(nocomunas)

"""Extraemos las columnas usando nuestro set de `nocolumnas`:"""

comunas = []
for i in range(len(seq2)):
  if i not in nocomunas:
    comunas.append(seq2[i])

"""Podemos ver ahora nuestras comunas:"""

print(comunas)

"""Sin embargo, notamos que como el decoding a `utf-8` no funcionó (usé los códigos de clase... fome que no haya funcionado, busqué mucho rato en Google y no pillé solución), cambiaremos a mano los nombres de las comunas que están mal escritas, de manera tal de simplificar la futura georreferenciación con nuestro GeoDataFrame de Arcos."""

comunas[2] = 'Ñuñoa'
comunas[4] = 'Peñalolen'
comunas[9] = 'Estación Central'
comunas[10] = 'Maipú'
comunas[15] = 'San Joaquín'
comunas[18] = 'Conchalí'

"""Notamos que ahora tenemos las comunas bien escritas en nuestra lista ordenada de comunas."""

print(comunas)

"""Para luego hacer más simple el traslape de estos datos a mi GeoDataFrame de Arcos, crearé un diccionario que contenga esta información."""

dicapruebo = dict()
for i in range(len(comunas)):
  dicapruebo[comunas[i]] = apruebo[i]

"""#### Llevando información *Scrapeada* a `zonas`.

Ahora siguiendo pasos parecidos a los que se han usado previamente, procedemos a asignar a cada zona según la comuna que pertenezca, el porcentaje de votación de la opción `Apruebo`. Se asigna esta información a zonas y no a otro GeoDF, dado que como los datos son por comuna, es lo que tiene más sentido.

Creamos entonces una función que reciba la comuna de una zona y que entregue de vuelta el porcentaje de votación por la opción apruebo que hubo en esa zona. Si la comuna no tiene información del porcentaje de votación de la opción apruebo, entonces se rellena con la media.
"""

def encontrar_apruebo(comuna_zona, diccionario = dicapruebo):
  try:
    return diccionario[comuna_zona]
  except:
    return np.mean(apruebo)

zonas['apruebo'] = zonas['Comuna'].apply(lambda x: encontrar_apruebo(x))

zonas.head()

"""Notamos que efectivamente se agregó de manera correcta la columna apruebo con los porcentajes respectivos de cada zona."""

plt.figure(figsize = (10,10))
zonas.plot(column = 'apruebo', legend = True)
plt.show()

"""Es impactante ver como se distribuyen los votos en Santiago. Mucho se ha dicho al respecto en los últimos días. No es el objetivo de este estudio juvenil, hacer comentarios pero es importante que como chilenos reflexionemos al respecto

Apliquemos log a la columna apruebo, haber si logramos tener menos cambios bruscos de color.
"""

import seaborn as sns

zonasC = zonas.copy()
zonasC['apruebo'] = np.log(zonas['apruebo'])
zonasC.plot(column = 'apruebo', legend = True)
plt.show()

"""Algo cambia, pero aún se puede ver la gran diferencia de votación entre los distintos sectores de Santiago. Probablemente, la aplicación de log no ayuda mucho, ya que la diferencia entre los porcentajes de votación fue abismal.

## Misión 4: Indicadores en Transporte Público

En esta última misión deberá utilizar el Shape de los trazados de los servicios de Red (ex-Transantiago) y generar un ranking con ellos. El ranking debe ser realizado a en base a un indicador construido por usted. El único requisito en la construcción del indicador es que se utilice la información obtenida en las misiones M2 y M3. Debe pasar la información a los servicios y determinar el indicador en cada uno de ellos. Finalmente cree una visualización de mapa de este indicador para los servicios de Red.

Obtenemos primero entonces el shape de Red. Y lo visualizamos con el objetivo de familiarizarnos un poco con su contenido.
"""

df_red = gpd.read_file('Trazados_red.shp')

df_red.head()

"""Luego de la visualización notamos que tiene diversas columnas, podemos notar rápidamente que las coordenadas no soon del tipo `lat-long`. 

Veamos como se ve en el mapa. 
"""

df_red.plot(color = "tomato")

"""Chequiemos rápidamente cual es el código EPSG de esta archivo .shp:"""

df_red.crs

"""El archivo tiene un código EPSG de 32719, como pudimos notar al ver el head de este nuevo GeoDF creado, la georreferenciación se hizo con otro código al que habíamos utilizado en las demás georreferenciaciones utilizadas (EPSG: 4326). Convertimos entonces la georreferenciación a una del tipo 4326, mediante el siguiente código:"""

red = df_red.to_crs(4326)

"""Revisamos que se haya realizado nuestro objetivo:"""

red.head()

"""En lo que viene se hará lo siguiente:

Tenemos distintos recorridos (linestrings) que pasan por las distintas zonas y comunas de Santiago. En cada una de estas zonas sabemos cual es el porcentaje de votación de apruebo y también sabemos cual es la velocidad promedio de las calles de esa zona (no para los 4 horarios porque nos falló la función `velocidad`). 

Para cada zona elaboraremos el índice: **`ganas_de_cambio`**. Se cree que una persona que vive en una zona con velocidad promedio menor tiene más ganas de que cambien las cosas. Además, el hecho de que haya votado apruebo nos señala que esa persona tiene grandes ganas de que cambien las cosas también. Dado este razonamiento, el índice **ganas_de_cambio** se compondrá de la siguiente manera:

`ganas de cambio` = `apruebo` / `velocidad`

Este índice captura las ganas de que cambien las cosas que se vive o se tiene en cada zona. Dicho eso, después de obtener el valor de nuestro índice para cada zona, procederemos a asignarlos a los trazados de Red de la siguiente forma:

Se verá cada una de las zonas por las cuales atraviesa cada uno de los trazados de Red. Para cada uno de estos trazados, obtendremos el promedio de nuestro índice `ganas_de_cambio` entre todas las zonas por las cuales pase el trazado.

Teniendo este último valor promedio, hacemos el ranking pedido.

Partamos:

Debemos inicialmente obtener nuestro índice `ganas_de_cambio` para cada una de las zonas. Para ello *reciclaremos* la tabla `listoco` que contiene como índice el ID de cada zona y la velocidad promedio de los arcos de cada zona (solo para un horario, dado que nuestra función `velocidad` no funcionó).
"""

listoco.head()

"""Ahora agregamos a `listoco`, el porcentaje de personas de la comuna a la cual pertenece que votó apruebo."""

listoco["apruebo"] = listoco['Comuna'].apply(lambda x: encontrar_apruebo(x))

listoco.head()

"""Teniendo listo nuestro GeoDF, podemos calcular ahora nuestro índice:
`g_d_c` (corresponde al abreviativo de ganas de cambio).
"""

listoco['g_d_c'] = listoco['apruebo']/listoco['speed_kph']

"""Observamos que se haya cumplido según lo pensado."""

listoco.head()

"""Aprovechamos de visualizar la distribución de las zonas de nuestro índice de g_d_c."""

listoco.plot(column = 'g_d_c', legend = True, figsize = (20,20))

"""Nuevamente podemos ver la división entre los dos sectores de Santiago. Algo bastante propio de nuestro país, que se ha acentuado extremadamente en el último tiempo. El sector oriente de Santiago, destaca por sus pocas **ganas de cambio** en comparación al resto de Santiago. Esta figura es realmente impactante.

Procedemos ahora con el siguiente paso. Este sería ver a través de que zonas cruza cada linestring de los trazados de Red. Y luego calcular el promedio de nuestro índice. Para ello usaremos una técnica bastante parecida a la que usamos para obtener los vecinos de cada zona en la M2.

Para ello usaremos el diccionario creado en la misión 2: `diccio` que contiene el ID de cada zona como llave y como valor su valor su geometría respectiva.
"""

print(diccio)

"""Luego creamos la función `encontrar_gdc`, que recibe la geometría de un trazado en particular de Red, nuestro diccio y `listoco`. Esta función encuentra primero todas las zonas por las cuales cruza el trazado de Red y luego obtiene el promedio de todas los valores de **ganas_de_cambio**, de las zonas por las cuales pasa el trazado. Este último valor es el que retorna."""

def encontrar_gdc(trazado, dicciozonas = diccio, df = listoco):
  zonas = []
  for id in dicciozonas.keys():
    if trazado.intersects(dicciozonas[id]):
      zonas.append(int(id))
  #print(zonas)
  sumagdc = 0
  div = 0
  for zona in zonas:
    div += 1
    sumagdc += df.loc[df.index == zona, 'g_d_c'].values[0]
  #print(sumagdc)
  #print(div)
  return (sumagdc / div)

"""Probamos que funciones agregando ciertos prints, para entender su lógica más claramente. Notamos que funciona! Podemos proceder a aplicar esta función a todo nuestro GeoDF, `red`."""

encontrar_gdc(red.loc[0, 'geometry'])

red["g_d_c"] = red['geometry'].apply(lambda x: encontrar_gdc(x))

red.head()

"""Notamos que tenemos una nueva columna de `g_d_c` en nuestro GeoDF de `red`. Debemos ahora crear un ranking. Para ello podemos ordenar la base de datos según su valor de `g_d_c` de manera descendente. Así podremos ver los trazados que probablemente lleven más gente con mayor ganas de cambio, más arriba y los menos más abajo."""

red_ordenada = red.sort_values(by=['g_d_c'])

red_ordenada.head()

"""Armamos el ranking, de los trazados de mayor a menor valor de **ganas_de_cambio**. El ranking esta completo. Se puede notar que tiene 1478 lugares, al igual que el número de observaciones del GeoDF `red`."""

lugar = 0
for trazado in red_ordenada.index:
  lugar += 1
  nombre_ruta = red_ordenada.loc[red_ordenada.index == trazado, 'ROUTE_NAME'].values[0]
  print(str(lugar) + '.- '+ str(nombre_ruta))

"""Si bien lo ideal hubiese sido obtener el valor de g_d_c, con las velocidades promedio entre los dos horarios que implican tiempos de traslado desde o hacia el trabajo (horarios 2 y 4) usando `arcos_final_horario2` y `arcos_final_horario4` , esto no se pudo realizar porque no funcionó nuestra función ´velocidad´ en la M.2b. La lógica para hacer el ranking sería la misma, obviamente.

Ahora generamos la visualización pedida, para ello usamos el método plot, sobre red y coloreamos según la columna `g_d_c`. Hacemos este plot sobre el plot del GeoDF de `revisar`, que contiene solo las zonas que tenían nodos de vialidades (de OSMNX) ubicados dentro de Santiago urbano. Si bien, hay algunos trazado que salen de estas zonas (especialmente a la izquierda del mapa impreso) se puede notar más claramente donde pasan nuestros trazados que llevan gente con más **ganas de cambio** y los trazados que llevan menos gente con ganas de cambio.
"""

ax = revisar.plot(color = "white", edgecolor = "grey", figsize = (20,20))
red.plot(ax = ax, column = 'g_d_c', legend = True, figsize = (20,20))
plt.axis('off')
plt.show()

"""Nuevamente, no es la idea ponerme político. Pero llama nuevamente la atención, donde se ubican los trazados que llevan gente con más ganas de cambio y los que llevan menos gente con ganas de cambio. Al parecer el sector oriente esta poblado por gente con pocas ganas de cambio y el sector poniente de Santiago tiene muchas muchas ganas de cambio. Esto es señal de la polarización vivida en el país, la podemos ver a través del resultado de la votación del plebiscito y la visualización realizada en la M3 y ahora con nuestro índice de ganas de cambio en sus trazados de Red, podemos ver esta misma división que se vive hoy en día en Santiago.

## Cierre: 

Estimado Ayudante: Muchas gracias por corregir cada uno de mis trabajos tal lateros y largos. Aprendí demasiado y quedo eternamente agradecido con todo el cuerpo docente. Me alegraron bastante este semestre de encierro, Gracias totales!
"""